# avito-ds-internship-task

## Whitespace Correction

## Описание

В результате поиска доступных решений я нашел статью ["Fast whitespace correction with encoder-only transformers"](https://aclanthology.org/2023.acl-demo.37.pdf), в которой авторы решали аналогичную проблему для английского языка. Протестировав их решение, я пришел к выводу, что результаты на русском языке не впечатляют. В этом проекте я реализовал решения для коррекции текста на русском языке. Проанализировав статью, я решил остановиться на подходе Encoder Decoder. В качестве pretrain модели я взял ByT5 ED — это версия модели T5 без токенизатора, предназначенная для работы непосредственно с необработанными байтами UTF-8. Это означает, что она может обрабатывать любой язык, более устойчива к помехам, таким как опечатки, и проще в использовании, поскольку не требует конвейера предварительной обработки. Для более подробного ознакомления предлагаю пройтись по ноутбукам в репозитории.

Данные: [zarus03/avito-ds-dataset](https://www.kaggle.com/datasets/zarus03/avito-ds-dataset)

Веса модели: [zarus03/byt5-wsc](https://huggingface.co/zarus03/byt5-wsc)

Стек: Python, PyTorch, NumPy, Pandas, Transformers

## Обзор статьи

В статье описывается два подхода. Первый EO (Encoder Only) рассматривает проблему как задачу маркировки последовательностей, где модель прогнозирует один из трех токенов исправления $R = \{K,I,D\}$ для каждого символа $x_i$ во входной последовательности. Второй ED (Encoder Decoder) использует модель Transformer с линейным выходным слоем, обученным преобразовывать последовательности символов с ошибками пробелов в последовательности без ошибок пробелов, выводя символы один за другим. На каждом шаге вывода t мы используем декодер ED для прогнозирования распределения вероятностей по алфавиту с учётом входной последовательности и предыдущих выходных данных $y_{<t} = (y_1,...,y_{t−1})$. Чтобы гарантировать, что модель ED изменяет только пробелы в последовательности во время вывода, авторы ограничивают набор возможных выходных данных на каждом шаге символом пробела или следующим символом из входной последовательности $x_j$.

Оба подхода, EO и ED, обучаются на входных последовательностях, содержащих до 512 токенов, и ограничены ими. Поскольку длина абзацев в реальном мире часто превышает это ограничение, авторы используют метод скользящего окна при выводе, разбивая входные последовательности на окна по 384 токена и добавляя 64 токена слева и справа от окна в качестве дополнительного контекста.

Для создания пар правильно и неправильно написанных последовательностей авторы вводят в абзацы ошибки, связанные с пробелами:
1. В 10% абзацев удаляют все пробелы.
2. В 10% абзацев добавляют пробел между каждой парой смежных символов.
3. В оставшихся 80% абзацев вставляют пробел между двумя смежными символами, не являющимися пробелами, с вероятностью 10% и удаляют существующий пробел с вероятностью 20%.

## Структура проекта

```txt
.
├── data
├── notebooks
│   ├── analyze_data.ipynb
│   ├── data_preprocessing.ipynb
│   ├── model_training.ipynb
│   └── test_existing_solutions.ipynb
├── README.md
├── src
│   ├── logger.py
│   └── predict.py
└── static
```

Советую изучать в следующем порядке:

1. `test_existing_solutions.ipynb` - анализ существующих решений
2. `analyze_data.ipynb` - анализ тестовых данных для формирования выборки для обучения
3. `data_preprocessing.ipynb` - сбор и обработка данных для тренировки модели
4. `model_training.ipynb` - код для тренировки модели
5. `src/predict.py` - скрипт для запуска воспроизведения результатов

## Метрики

Для вычисления space метрик используются позиции пробелов в корректной и сгенерированой строках.

| Dataset | Character Accuracy | Space Precision | Space Recall | Space F1 |
|----------|----------|----------|----------|----------|
| [russian-literature](https://www.kaggle.com/datasets/d0rj3228/russian-literature) | 0.767300 | 0.943574 | 0.944903 | 0.943664  |

## Вычислительные мощности 

Обучение проводилось на платформе Kaggle GPU P100 (GPU Memory 16 GiB), RAM 29 GiB

```     
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |
| N/A   30C    P0             26W /  250W |       0MiB /  16384MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
```

## Идеи для улучшения

- Использовать beam search

## Источники

1. Bast, Hannah, Matthias Hertel, and Sebastian Walter. "Fast whitespace correction with encoder-only transformers." Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations). 2023.

2. Открытая реализация для английского языка:
[whitespace-correction](https://github.com/ad-freiburg/whitespace-correction)
