{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "426850c4",
   "metadata": {},
   "source": [
    "# Тренировка модели машинного обучения\n",
    "\n",
    "Запускал ноутбук удаленно на Kaggle Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e2e23d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep 22 11:13:54 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   30C    P0             26W /  250W |       0MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e5acfc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d0d696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 11:14:15.763343: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758539655.941030      61 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758539655.991331      61 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    LogitsProcessor,\n",
    "    LogitsProcessorList,\n",
    ")\n",
    "import getpass\n",
    "os.environ[\"HF_TOKEN\"] =  getpass.getpass(\"Enter your HF_TOKEN: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ab72c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_window_length = 60\n",
    "stride = 45\n",
    "gen_max_length = 128\n",
    "seed = 1024\n",
    "val_size = 0.1\n",
    "max_examples = 100000\n",
    "# 0.1 * 100000 = 10000 / 10000 // 8 = 1250 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e57f77cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdad757",
   "metadata": {},
   "source": [
    "## Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c6a4096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_with_spaces(text):\n",
    "    method_choice = random.random()\n",
    "    \n",
    "    if method_choice < 0.1: # 10% - Remove all spaces\n",
    "        return text.replace(\" \", \"\")\n",
    "    \n",
    "    elif method_choice < 0.2: # 10% - Add space between each character\n",
    "        return \" \".join(text)\n",
    "    \n",
    "    else:\n",
    "        result_chars = []\n",
    "        \n",
    "        i = 0\n",
    "        n = len(text)\n",
    "    \n",
    "        while i < n:\n",
    "            current_char = text[i]\n",
    "            if current_char == ' ':\n",
    "                # It's a space. 20% chance to delete it (skip it).\n",
    "                if random.random() < 0.2:\n",
    "                    i += 1  # Just move to the next char, don't add this space.\n",
    "                    continue\n",
    "                else:\n",
    "                    result_chars.append(current_char)  # Keep the space.\n",
    "                    i += 1\n",
    "            else:\n",
    "                # It's a non-space character. Always add it.\n",
    "                result_chars.append(current_char)\n",
    "                # Check if there is a next character and it's also non-space\n",
    "                if i + 1 < n and text[i+1] != ' ':\n",
    "                    # 10% chance to insert an extra space after this character.\n",
    "                    if random.random() < 0.1:\n",
    "                        result_chars.append(' ')\n",
    "                i += 1\n",
    "\n",
    "    return ''.join(result_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9ee60df",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_filepath = '/kaggle/input/avito-ds-dataset/training_data_russian_literature.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75e228be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(training_data_filepath, 'r') as file:\n",
    "    text = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b65679f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e675c77d724045953375183ed6f945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/819761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "src_windows = []\n",
    "tgt_windows = []\n",
    "\n",
    "for i in tqdm(range(0, len(text), stride)):\n",
    "    src_window = text[i:i+data_window_length].strip()\n",
    "    # Generate the corrupted version of that window\n",
    "    tgt_window = process_text_with_spaces(src_window).strip()\n",
    "    \n",
    "    if len(src_window) > 0 and len(tgt_window) > 0:\n",
    "        src_windows.append(tgt_window)\n",
    "        tgt_windows.append(src_window)\n",
    "\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    \"src_text\": src_windows,\n",
    "    \"tgt_text\": tgt_windows,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bdddd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src='ом назовет,-- способностьныне неслыханная! Прежде,по крайн'\n",
      "tgt='ом назовет,-- способность ныне неслыханная! Прежде, по крайн'\n"
     ]
    }
   ],
   "source": [
    "idx = 60\n",
    "print(f\"src='{dataset['src_text'][idx]}'\")\n",
    "print(f\"tgt='{dataset['tgt_text'][idx]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "865ce28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['src_text', 'tgt_text'],\n",
       "    num_rows: 819761\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c69d2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['src_text', 'tgt_text'],\n",
       "    num_rows: 100000\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size = len(dataset)\n",
    "sample_size = min(dataset_size, max_examples)\n",
    "random_indices = random.sample(range(dataset_size), sample_size)\n",
    "dataset_limited = dataset.select(random_indices)\n",
    "dataset_limited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d1f5461",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split = dataset_limited.train_test_split(test_size=val_size, seed=seed)\n",
    "train_dataset = dataset_split[\"train\"]\n",
    "val_dataset = dataset_split[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a084aeb6",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62722af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a8d8c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e3b68a67904ba7acbeb761d392cfab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e8c3490fc44230a8562c37687157fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/3.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c9ced4b1ca241f98e697a875ea96931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/3.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a170b7ac16a9459ea8b1ec97f917979b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/797 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc5e0d2ab624ba1b2fa1b3cb007b4a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad067dddea6c4173a1fb3fedfa4301e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"zarus03/byt5-wsc\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71a0452e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(384, 1472)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(384, 1472)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1472, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=1472, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=1472, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=1472, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1472, out_features=3584, bias=False)\n",
       "              (wi_1): Linear(in_features=1472, out_features=3584, bias=False)\n",
       "              (wo): Linear(in_features=3584, out_features=1472, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1472, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=1472, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=1472, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=1472, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1472, out_features=3584, bias=False)\n",
       "              (wi_1): Linear(in_features=1472, out_features=3584, bias=False)\n",
       "              (wo): Linear(in_features=3584, out_features=1472, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(384, 1472)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1472, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=1472, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=1472, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=1472, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1472, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=1472, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=1472, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=1472, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1472, out_features=3584, bias=False)\n",
       "              (wi_1): Linear(in_features=1472, out_features=3584, bias=False)\n",
       "              (wo): Linear(in_features=3584, out_features=1472, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-3): 3 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1472, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=1472, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=1472, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=1472, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1472, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=1472, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=1472, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=1472, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1472, out_features=3584, bias=False)\n",
       "              (wi_1): Linear(in_features=1472, out_features=3584, bias=False)\n",
       "              (wo): Linear(in_features=3584, out_features=1472, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1472, out_features=384, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b54e57d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c0f58d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '</s>': 1,\n",
       " '<unk>': 2,\n",
       " '\\x00': 3,\n",
       " '\\x01': 4,\n",
       " '\\x02': 5,\n",
       " '\\x03': 6,\n",
       " '\\x04': 7,\n",
       " '\\x05': 8,\n",
       " '\\x06': 9,\n",
       " '\\x07': 10,\n",
       " '\\x08': 11,\n",
       " '\\t': 12,\n",
       " '\\n': 13,\n",
       " '\\x0b': 14,\n",
       " '\\x0c': 15,\n",
       " '\\r': 16,\n",
       " '\\x0e': 17,\n",
       " '\\x0f': 18,\n",
       " '\\x10': 19,\n",
       " '\\x11': 20,\n",
       " '\\x12': 21,\n",
       " '\\x13': 22,\n",
       " '\\x14': 23,\n",
       " '\\x15': 24,\n",
       " '\\x16': 25,\n",
       " '\\x17': 26,\n",
       " '\\x18': 27,\n",
       " '\\x19': 28,\n",
       " '\\x1a': 29,\n",
       " '\\x1b': 30,\n",
       " '\\x1c': 31,\n",
       " '\\x1d': 32,\n",
       " '\\x1e': 33,\n",
       " '\\x1f': 34,\n",
       " ' ': 35,\n",
       " '!': 36,\n",
       " '\"': 37,\n",
       " '#': 38,\n",
       " '$': 39,\n",
       " '%': 40,\n",
       " '&': 41,\n",
       " \"'\": 42,\n",
       " '(': 43,\n",
       " ')': 44,\n",
       " '*': 45,\n",
       " '+': 46,\n",
       " ',': 47,\n",
       " '-': 48,\n",
       " '.': 49,\n",
       " '/': 50,\n",
       " '0': 51,\n",
       " '1': 52,\n",
       " '2': 53,\n",
       " '3': 54,\n",
       " '4': 55,\n",
       " '5': 56,\n",
       " '6': 57,\n",
       " '7': 58,\n",
       " '8': 59,\n",
       " '9': 60,\n",
       " ':': 61,\n",
       " ';': 62,\n",
       " '<': 63,\n",
       " '=': 64,\n",
       " '>': 65,\n",
       " '?': 66,\n",
       " '@': 67,\n",
       " 'A': 68,\n",
       " 'B': 69,\n",
       " 'C': 70,\n",
       " 'D': 71,\n",
       " 'E': 72,\n",
       " 'F': 73,\n",
       " 'G': 74,\n",
       " 'H': 75,\n",
       " 'I': 76,\n",
       " 'J': 77,\n",
       " 'K': 78,\n",
       " 'L': 79,\n",
       " 'M': 80,\n",
       " 'N': 81,\n",
       " 'O': 82,\n",
       " 'P': 83,\n",
       " 'Q': 84,\n",
       " 'R': 85,\n",
       " 'S': 86,\n",
       " 'T': 87,\n",
       " 'U': 88,\n",
       " 'V': 89,\n",
       " 'W': 90,\n",
       " 'X': 91,\n",
       " 'Y': 92,\n",
       " 'Z': 93,\n",
       " '[': 94,\n",
       " '\\\\': 95,\n",
       " ']': 96,\n",
       " '^': 97,\n",
       " '_': 98,\n",
       " '`': 99,\n",
       " 'a': 100,\n",
       " 'b': 101,\n",
       " 'c': 102,\n",
       " 'd': 103,\n",
       " 'e': 104,\n",
       " 'f': 105,\n",
       " 'g': 106,\n",
       " 'h': 107,\n",
       " 'i': 108,\n",
       " 'j': 109,\n",
       " 'k': 110,\n",
       " 'l': 111,\n",
       " 'm': 112,\n",
       " 'n': 113,\n",
       " 'o': 114,\n",
       " 'p': 115,\n",
       " 'q': 116,\n",
       " 'r': 117,\n",
       " 's': 118,\n",
       " 't': 119,\n",
       " 'u': 120,\n",
       " 'v': 121,\n",
       " 'w': 122,\n",
       " 'x': 123,\n",
       " 'y': 124,\n",
       " 'z': 125,\n",
       " '{': 126,\n",
       " '|': 127,\n",
       " '}': 128,\n",
       " '~': 129,\n",
       " '\\x7f': 130,\n",
       " '\\x80': 131,\n",
       " '\\x81': 132,\n",
       " '\\x82': 133,\n",
       " '\\x83': 134,\n",
       " '\\x84': 135,\n",
       " '\\x85': 136,\n",
       " '\\x86': 137,\n",
       " '\\x87': 138,\n",
       " '\\x88': 139,\n",
       " '\\x89': 140,\n",
       " '\\x8a': 141,\n",
       " '\\x8b': 142,\n",
       " '\\x8c': 143,\n",
       " '\\x8d': 144,\n",
       " '\\x8e': 145,\n",
       " '\\x8f': 146,\n",
       " '\\x90': 147,\n",
       " '\\x91': 148,\n",
       " '\\x92': 149,\n",
       " '\\x93': 150,\n",
       " '\\x94': 151,\n",
       " '\\x95': 152,\n",
       " '\\x96': 153,\n",
       " '\\x97': 154,\n",
       " '\\x98': 155,\n",
       " '\\x99': 156,\n",
       " '\\x9a': 157,\n",
       " '\\x9b': 158,\n",
       " '\\x9c': 159,\n",
       " '\\x9d': 160,\n",
       " '\\x9e': 161,\n",
       " '\\x9f': 162,\n",
       " '\\xa0': 163,\n",
       " '¡': 164,\n",
       " '¢': 165,\n",
       " '£': 166,\n",
       " '¤': 167,\n",
       " '¥': 168,\n",
       " '¦': 169,\n",
       " '§': 170,\n",
       " '¨': 171,\n",
       " '©': 172,\n",
       " 'ª': 173,\n",
       " '«': 174,\n",
       " '¬': 175,\n",
       " '\\xad': 176,\n",
       " '®': 177,\n",
       " '¯': 178,\n",
       " '°': 179,\n",
       " '±': 180,\n",
       " '²': 181,\n",
       " '³': 182,\n",
       " '´': 183,\n",
       " 'µ': 184,\n",
       " '¶': 185,\n",
       " '·': 186,\n",
       " '¸': 187,\n",
       " '¹': 188,\n",
       " 'º': 189,\n",
       " '»': 190,\n",
       " '¼': 191,\n",
       " '½': 192,\n",
       " '¾': 193,\n",
       " '¿': 194,\n",
       " 'À': 195,\n",
       " 'Á': 196,\n",
       " 'Â': 197,\n",
       " 'Ã': 198,\n",
       " 'Ä': 199,\n",
       " 'Å': 200,\n",
       " 'Æ': 201,\n",
       " 'Ç': 202,\n",
       " 'È': 203,\n",
       " 'É': 204,\n",
       " 'Ê': 205,\n",
       " 'Ë': 206,\n",
       " 'Ì': 207,\n",
       " 'Í': 208,\n",
       " 'Î': 209,\n",
       " 'Ï': 210,\n",
       " 'Ð': 211,\n",
       " 'Ñ': 212,\n",
       " 'Ò': 213,\n",
       " 'Ó': 214,\n",
       " 'Ô': 215,\n",
       " 'Õ': 216,\n",
       " 'Ö': 217,\n",
       " '×': 218,\n",
       " 'Ø': 219,\n",
       " 'Ù': 220,\n",
       " 'Ú': 221,\n",
       " 'Û': 222,\n",
       " 'Ü': 223,\n",
       " 'Ý': 224,\n",
       " 'Þ': 225,\n",
       " 'ß': 226,\n",
       " 'à': 227,\n",
       " 'á': 228,\n",
       " 'â': 229,\n",
       " 'ã': 230,\n",
       " 'ä': 231,\n",
       " 'å': 232,\n",
       " 'æ': 233,\n",
       " 'ç': 234,\n",
       " 'è': 235,\n",
       " 'é': 236,\n",
       " 'ê': 237,\n",
       " 'ë': 238,\n",
       " 'ì': 239,\n",
       " 'í': 240,\n",
       " 'î': 241,\n",
       " 'ï': 242,\n",
       " 'ð': 243,\n",
       " 'ñ': 244,\n",
       " 'ò': 245,\n",
       " 'ó': 246,\n",
       " 'ô': 247,\n",
       " 'õ': 248,\n",
       " 'ö': 249,\n",
       " '÷': 250,\n",
       " 'ø': 251,\n",
       " 'ù': 252,\n",
       " 'ú': 253,\n",
       " 'û': 254,\n",
       " 'ü': 255,\n",
       " 'ý': 256,\n",
       " 'þ': 257,\n",
       " 'ÿ': 258,\n",
       " '<extra_id_0>': 259,\n",
       " '<extra_id_1>': 260,\n",
       " '<extra_id_2>': 261,\n",
       " '<extra_id_3>': 262,\n",
       " '<extra_id_4>': 263,\n",
       " '<extra_id_5>': 264,\n",
       " '<extra_id_6>': 265,\n",
       " '<extra_id_7>': 266,\n",
       " '<extra_id_8>': 267,\n",
       " '<extra_id_9>': 268,\n",
       " '<extra_id_10>': 269,\n",
       " '<extra_id_11>': 270,\n",
       " '<extra_id_12>': 271,\n",
       " '<extra_id_13>': 272,\n",
       " '<extra_id_14>': 273,\n",
       " '<extra_id_15>': 274,\n",
       " '<extra_id_16>': 275,\n",
       " '<extra_id_17>': 276,\n",
       " '<extra_id_18>': 277,\n",
       " '<extra_id_19>': 278,\n",
       " '<extra_id_20>': 279,\n",
       " '<extra_id_21>': 280,\n",
       " '<extra_id_22>': 281,\n",
       " '<extra_id_23>': 282,\n",
       " '<extra_id_24>': 283,\n",
       " '<extra_id_25>': 284,\n",
       " '<extra_id_26>': 285,\n",
       " '<extra_id_27>': 286,\n",
       " '<extra_id_28>': 287,\n",
       " '<extra_id_29>': 288,\n",
       " '<extra_id_30>': 289,\n",
       " '<extra_id_31>': 290,\n",
       " '<extra_id_32>': 291,\n",
       " '<extra_id_33>': 292,\n",
       " '<extra_id_34>': 293,\n",
       " '<extra_id_35>': 294,\n",
       " '<extra_id_36>': 295,\n",
       " '<extra_id_37>': 296,\n",
       " '<extra_id_38>': 297,\n",
       " '<extra_id_39>': 298,\n",
       " '<extra_id_40>': 299,\n",
       " '<extra_id_41>': 300,\n",
       " '<extra_id_42>': 301,\n",
       " '<extra_id_43>': 302,\n",
       " '<extra_id_44>': 303,\n",
       " '<extra_id_45>': 304,\n",
       " '<extra_id_46>': 305,\n",
       " '<extra_id_47>': 306,\n",
       " '<extra_id_48>': 307,\n",
       " '<extra_id_49>': 308,\n",
       " '<extra_id_50>': 309,\n",
       " '<extra_id_51>': 310,\n",
       " '<extra_id_52>': 311,\n",
       " '<extra_id_53>': 312,\n",
       " '<extra_id_54>': 313,\n",
       " '<extra_id_55>': 314,\n",
       " '<extra_id_56>': 315,\n",
       " '<extra_id_57>': 316,\n",
       " '<extra_id_58>': 317,\n",
       " '<extra_id_59>': 318,\n",
       " '<extra_id_60>': 319,\n",
       " '<extra_id_61>': 320,\n",
       " '<extra_id_62>': 321,\n",
       " '<extra_id_63>': 322,\n",
       " '<extra_id_64>': 323,\n",
       " '<extra_id_65>': 324,\n",
       " '<extra_id_66>': 325,\n",
       " '<extra_id_67>': 326,\n",
       " '<extra_id_68>': 327,\n",
       " '<extra_id_69>': 328,\n",
       " '<extra_id_70>': 329,\n",
       " '<extra_id_71>': 330,\n",
       " '<extra_id_72>': 331,\n",
       " '<extra_id_73>': 332,\n",
       " '<extra_id_74>': 333,\n",
       " '<extra_id_75>': 334,\n",
       " '<extra_id_76>': 335,\n",
       " '<extra_id_77>': 336,\n",
       " '<extra_id_78>': 337,\n",
       " '<extra_id_79>': 338,\n",
       " '<extra_id_80>': 339,\n",
       " '<extra_id_81>': 340,\n",
       " '<extra_id_82>': 341,\n",
       " '<extra_id_83>': 342,\n",
       " '<extra_id_84>': 343,\n",
       " '<extra_id_85>': 344,\n",
       " '<extra_id_86>': 345,\n",
       " '<extra_id_87>': 346,\n",
       " '<extra_id_88>': 347,\n",
       " '<extra_id_89>': 348,\n",
       " '<extra_id_90>': 349,\n",
       " '<extra_id_91>': 350,\n",
       " '<extra_id_92>': 351,\n",
       " '<extra_id_93>': 352,\n",
       " '<extra_id_94>': 353,\n",
       " '<extra_id_95>': 354,\n",
       " '<extra_id_96>': 355,\n",
       " '<extra_id_97>': 356,\n",
       " '<extra_id_98>': 357,\n",
       " '<extra_id_99>': 358,\n",
       " '<extra_id_100>': 359,\n",
       " '<extra_id_101>': 360,\n",
       " '<extra_id_102>': 361,\n",
       " '<extra_id_103>': 362,\n",
       " '<extra_id_104>': 363,\n",
       " '<extra_id_105>': 364,\n",
       " '<extra_id_106>': 365,\n",
       " '<extra_id_107>': 366,\n",
       " '<extra_id_108>': 367,\n",
       " '<extra_id_109>': 368,\n",
       " '<extra_id_110>': 369,\n",
       " '<extra_id_111>': 370,\n",
       " '<extra_id_112>': 371,\n",
       " '<extra_id_113>': 372,\n",
       " '<extra_id_114>': 373,\n",
       " '<extra_id_115>': 374,\n",
       " '<extra_id_116>': 375,\n",
       " '<extra_id_117>': 376,\n",
       " '<extra_id_118>': 377,\n",
       " '<extra_id_119>': 378,\n",
       " '<extra_id_120>': 379,\n",
       " '<extra_id_121>': 380,\n",
       " '<extra_id_122>': 381,\n",
       " '<extra_id_123>': 382,\n",
       " '<extra_id_124>': 383}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa73f27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(custom_str)=6\n",
      "len(tokenized_seq)=13\n",
      "[211, 194, 212, 131, 211, 187, 211, 181, 211, 184, 212, 133, 1]\n"
     ]
    }
   ],
   "source": [
    "custom_str = 'привет'\n",
    "print(f\"{len(custom_str)=}\")\n",
    "tokenized_seq = tokenizer.encode(custom_str)\n",
    "print(f\"{len(tokenized_seq)=}\")\n",
    "print(tokenized_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61029132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch):\n",
    "    model_inputs = tokenizer(\n",
    "        batch[\"src_text\"],\n",
    "        max_length=gen_max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = tokenizer(\n",
    "        batch[\"tgt_text\"],\n",
    "        max_length=gen_max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7f25f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45442a64b25f4efc8431cfbef03dab39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc28988a5f824a0f80d2fd9cde8cc029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = train_dataset.map(\n",
    "    preprocess, batched=True, remove_columns=[\"src_text\", \"tgt_text\"])\n",
    "\n",
    "tokenized_val = val_dataset.map(\n",
    "    preprocess, batched=True, remove_columns=[\"src_text\", \"tgt_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e95c9e",
   "metadata": {},
   "source": [
    "## Constrained decoding implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ac3ee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 4) Constrained decoding: custom LogitsProcessor\n",
    "# ---------------------------\n",
    "# We'll implement a LogitsProcessor that, for each batch element at each generation step,\n",
    "# allows only two token ids:\n",
    "#   - space_id\n",
    "#   - the id for the next byte of the encoder input sequence (ignoring encoder padding)\n",
    "# Implementation details:\n",
    "# - The processor is initialized with encoder_input_ids (unpadded list of bytes) for each batch item.\n",
    "# - At each call, given input_ids (generated tokens so far), we compute pos = number of generated tokens that matched encoder bytes in order.\n",
    "#   - Spaces do not advance pos. When pos reaches encoder length, generation stops (or only spaces allowed).\n",
    "# - Then we set logits for disallowed tokens to -inf.\n",
    "\n",
    "class EDConstrainedLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, encoder_inputs: List[List[int]], pad_token_id: int, space_token_id: int, eos_token_id: int):\n",
    "        \"\"\"\n",
    "        encoder_inputs: list of lists (for each batch element) containing encoder input ids \n",
    "        (without special padding or with padding - we'll trim)\n",
    "        \"\"\"\n",
    "        self.encoder_inputs = [self._trim(inp, pad_token_id) for inp in encoder_inputs]\n",
    "        self.pad = pad_token_id\n",
    "        self.space_id = space_token_id\n",
    "        self.eos_id = eos_token_id\n",
    "\n",
    "    def _trim(self, arr, pad_id):\n",
    "        # remove trailing pad tokens if present\n",
    "        trimmed = [int(x) for x in arr if int(x) != pad_id]\n",
    "        return trimmed\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        input_ids: (batch_size, cur_len)\n",
    "        scores: (batch_size, vocab_size) (logits for next token)\n",
    "        Return masked scores.\n",
    "        \"\"\"\n",
    "        bs = input_ids.size(0)\n",
    "        vocab_size = scores.size(-1)\n",
    "        device = scores.device\n",
    "        big_neg = -1e9\n",
    "\n",
    "        masked = scores.clone()\n",
    "\n",
    "        for i in range(bs):\n",
    "            enc = self.encoder_inputs[i]\n",
    "            gen = input_ids[i].tolist()\n",
    "            \n",
    "            # compute position in encoder: count how many times we consumed a source token\n",
    "            pos = 0\n",
    "            for tok in gen:\n",
    "                if pos < len(enc) and tok == enc[pos]:\n",
    "                    pos += 1\n",
    "\n",
    "            allowed = {self.space_id}\n",
    "            if pos < len(enc):\n",
    "                allowed.add(enc[pos])\n",
    "            if pos >= len(enc):\n",
    "                # Allow EOS after consuming all input\n",
    "                allowed.add(self.eos_id)\n",
    "            \n",
    "            # Create mask\n",
    "            mask = torch.full((vocab_size,), big_neg, device=device)\n",
    "            allowed_list = list(allowed)\n",
    "            mask[allowed_list] = 0.0\n",
    "            masked[i] = scores[i] + mask\n",
    "\n",
    "        return masked\n",
    "\n",
    "def build_logits_processor_for_batch(batch_input_ids, pad_token_id, space_token_id, eos_token_id):\n",
    "    # batch_input_ids: torch tensor (batch, seq_len)\n",
    "    encoder_inputs = [row.tolist() for row in batch_input_ids]\n",
    "    ed = EDConstrainedLogitsProcessor(\n",
    "        encoder_inputs=encoder_inputs,\n",
    "        pad_token_id=pad_token_id,\n",
    "        space_token_id=space_token_id,\n",
    "        eos_token_id=eos_token_id,\n",
    "    )\n",
    "    return LogitsProcessorList([ed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46de791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 7) Custom generate wrapper for constrained decoding\n",
    "# ---------------------------\n",
    "def generate_constrained(batch, max_length):\n",
    "    \"\"\"\n",
    "    batch: dict with 'input_ids' and 'attention_mask' (tensors, batched)\n",
    "    Return: list of decoded strings\n",
    "    \"\"\"\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    space_id = tokenizer.encode(\" \", add_special_tokens=False)[0]\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "\n",
    "    logits_processor = build_logits_processor_for_batch(\n",
    "        input_ids.detach().cpu(),\n",
    "        pad_token_id=pad_id,\n",
    "        space_token_id=space_id,\n",
    "        eos_token_id=eos_id,\n",
    "    )\n",
    "\n",
    "    generated = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length,\n",
    "        logits_processor=logits_processor,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=False,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    return generated.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e196ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 8) Seq2SeqTrainer but override prediction_step to use constrained decoding at eval time\n",
    "# ---------------------------\n",
    "class ConstrainedSeq2SeqTrainer(Seq2SeqTrainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.max_length = kwargs.pop('max_length', None)\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        \"\"\"\n",
    "        We override to use constrained generation at prediction time.\n",
    "        \"\"\"\n",
    "        # For training or when not predicting with generate, fallback\n",
    "        if not self.args.predict_with_generate or prediction_loss_only:\n",
    "            return super().prediction_step(model, inputs, prediction_loss_only, ignore_keys)\n",
    "\n",
    "        # otherwise, compute loss and also produce constrained predictions\n",
    "        # move inputs to device\n",
    "        has_labels = \"labels\" in inputs\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss = self.compute_loss(model, inputs, return_outputs=False)\n",
    "\n",
    "        generated_tokens = generate_constrained(inputs, self.max_length)\n",
    "\n",
    "        if generated_tokens.shape[-1] < self.max_length:\n",
    "            generated_tokens = self._pad_tensors_to_max_len(generated_tokens, self.max_length)\n",
    "\n",
    "        if has_labels:\n",
    "            labels = inputs[\"labels\"]\n",
    "            if labels.shape[-1] < self.max_length:\n",
    "                labels = self._pad_tensors_to_max_len(labels, self.max_length)\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        return (loss, generated_tokens, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad447167",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7dbfaa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_space_indices(text: str):\n",
    "    \"\"\"Return indices of spaces in the text\"\"\"\n",
    "    return {i for i, ch in enumerate(text) if ch.isspace()}\n",
    "\n",
    "\n",
    "def calculate_exact_match_accuracy(preds: list, labels: list) -> float:\n",
    "    \"\"\"Calculate exact string match accuracy\"\"\"\n",
    "    correct = sum(p == l for p, l in zip(preds, labels))\n",
    "    return correct / len(preds)\n",
    "\n",
    "\n",
    "def calculate_f1_score(preds: list, labels: list) -> dict:\n",
    "    \"\"\"Calculate F1 score and related metrics for space prediction\"\"\"\n",
    "    all_precisions, all_recalls, all_f1s = [], [], []\n",
    "\n",
    "    for pred, label in zip(preds, labels):\n",
    "        pred_spaces = get_space_indices(pred)\n",
    "        ref_spaces = get_space_indices(label)\n",
    "\n",
    "        tp = len(pred_spaces & ref_spaces)\n",
    "        fp = len(pred_spaces - ref_spaces)\n",
    "        fn = len(ref_spaces - pred_spaces)\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "        all_f1s.append(f1)\n",
    "    \n",
    "    return np.mean(all_precisions), np.mean(all_recalls), np.mean(all_f1s)\n",
    "\n",
    "    \n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    preds = preds[0]\n",
    "\n",
    "    # Convert logits to predicted token IDs\n",
    "    preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "    # preds is already numpy array (batch_size, seq_len)\n",
    "    # decode predictions\n",
    "    preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in labels with pad_token_id before decoding\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Exact string match accuracy\n",
    "    acc = calculate_exact_match_accuracy(preds, labels)\n",
    "    p, r, f1 = calculate_f1_score(preds, labels)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": p,\n",
    "        \"recall\": r,\n",
    "        \"f1\": f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3f6811",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3353995",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61/1764225300.py:7: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `ConstrainedSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9089' max='56250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 9089/56250 3:17:24 < 17:04:33, 0.77 it/s, Epoch 1.62/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>0.022558</td>\n",
       "      <td>0.756500</td>\n",
       "      <td>0.943370</td>\n",
       "      <td>0.946976</td>\n",
       "      <td>0.944508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.027900</td>\n",
       "      <td>0.022239</td>\n",
       "      <td>0.758200</td>\n",
       "      <td>0.941592</td>\n",
       "      <td>0.945187</td>\n",
       "      <td>0.942764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.025700</td>\n",
       "      <td>0.022106</td>\n",
       "      <td>0.759700</td>\n",
       "      <td>0.948346</td>\n",
       "      <td>0.952470</td>\n",
       "      <td>0.949739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.028200</td>\n",
       "      <td>0.021840</td>\n",
       "      <td>0.763400</td>\n",
       "      <td>0.944855</td>\n",
       "      <td>0.949971</td>\n",
       "      <td>0.946757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.021554</td>\n",
       "      <td>0.767300</td>\n",
       "      <td>0.943574</td>\n",
       "      <td>0.944903</td>\n",
       "      <td>0.943664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>0.022306</td>\n",
       "      <td>0.762000</td>\n",
       "      <td>0.946627</td>\n",
       "      <td>0.951034</td>\n",
       "      <td>0.948188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.021100</td>\n",
       "      <td>0.021837</td>\n",
       "      <td>0.766200</td>\n",
       "      <td>0.948647</td>\n",
       "      <td>0.950520</td>\n",
       "      <td>0.948975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.022359</td>\n",
       "      <td>0.761600</td>\n",
       "      <td>0.948227</td>\n",
       "      <td>0.955408</td>\n",
       "      <td>0.951119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.021579</td>\n",
       "      <td>0.765400</td>\n",
       "      <td>0.944654</td>\n",
       "      <td>0.948303</td>\n",
       "      <td>0.945862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_61/119358834.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training finished. Saving model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m                     )\n\u001b[1;32m   2554\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2555\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m                     if (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3789\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3791\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3793\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2551\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2553\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./byt5-ed\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_accumulation_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = ConstrainedSeq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    max_length=gen_max_length,\n",
    ")\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Training finished. Saving model...\")\n",
    "trainer.save_model(training_args.output_dir)\n",
    "\n",
    "print(f\"Model saved {training_args.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b90eef6",
   "metadata": {},
   "source": [
    "## Load checkpoint and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee983bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spaces(text: str, max_length: int = 128) -> str:\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        max_length=max_length\n",
    "    ).to(device)\n",
    "    \n",
    "    return [tokenizer.decode(seq, skip_special_tokens=True).strip()\n",
    "            for seq in generate_constrained(inputs, max_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d921c7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"zarus03/byt5-wsc\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b744a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Но вскоре возвратился с ружьём на перевес:'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_spaces('Новскоревозвратилсясружьёмнаперевес:')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7855c4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_no_spaces</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>куплюайфон14про</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ищудомвПодмосковье</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>сдаюквартирусмебельюитехникой</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>новыйдивандоставканедорого</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>отдамдаромкошку</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                 text_no_spaces\n",
       "0   0                куплюайфон14про\n",
       "1   1             ищудомвПодмосковье\n",
       "2   2  сдаюквартирусмебельюитехникой\n",
       "3   3     новыйдивандоставканедорого\n",
       "4   4                отдамдаромкошку"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('/kaggle/input/avito-ds-dataset/test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d684aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ace5ded50d4a95b1cb4386db330b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "correcting:   0%|          | 0/1005 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test['text_with_spaces'] = [correct_spaces(text)[0] \n",
    "                            for text in tqdm(test['text_no_spaces'], desc='correcting')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8c73c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_no_spaces</th>\n",
       "      <th>text_with_spaces</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>куплюайфон14про</td>\n",
       "      <td>куплю айфон 14 про</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ищудомвПодмосковье</td>\n",
       "      <td>ищу дом в Подмосковье</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>сдаюквартирусмебельюитехникой</td>\n",
       "      <td>сдаю квартиру с мебелью и техникой</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>новыйдивандоставканедорого</td>\n",
       "      <td>новый диван доставка недорого</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>отдамдаромкошку</td>\n",
       "      <td>отдам даром кошку</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>работавМосквеудаленно</td>\n",
       "      <td>работа в Москве удаленно</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>куплютелевизорPhilips</td>\n",
       "      <td>куплютелев изор Philips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>ищугрузчиковдляпереезда</td>\n",
       "      <td>ищу грузчиков для переезда</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>ремонтквартирподключ</td>\n",
       "      <td>ремонт квартир подключ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>куплюноутбукHP</td>\n",
       "      <td>куплюно утбук HP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>ищуквартирууметро</td>\n",
       "      <td>ищу квартиру умет ро</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>новаямикроволновкаSamsung</td>\n",
       "      <td>новаями кроволновка Samsung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>срочнопродамвелосипед</td>\n",
       "      <td>срочно продам велос и пед</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>куплюгитаруFender</td>\n",
       "      <td>куплю гитару Fender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>ищурепетиторапобиологии</td>\n",
       "      <td>ищу репетитора побиологии</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>сдаюгаражнадлительныйсрок</td>\n",
       "      <td>сдаю гараж надлительный срок</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>куплюдиванбу</td>\n",
       "      <td>куплю диван бу</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>ищумастерапоремонтухолодильников</td>\n",
       "      <td>ищу мастера по ремонту холодильников</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>новыйшкафдоставкасегодня</td>\n",
       "      <td>новый шкаф доставка сегодня</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>куплюXboxOne</td>\n",
       "      <td>куплю Xbox One</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                    text_no_spaces                      text_with_spaces\n",
       "0    0                   куплюайфон14про                    куплю айфон 14 про\n",
       "1    1                ищудомвПодмосковье                 ищу дом в Подмосковье\n",
       "2    2     сдаюквартирусмебельюитехникой    сдаю квартиру с мебелью и техникой\n",
       "3    3        новыйдивандоставканедорого         новый диван доставка недорого\n",
       "4    4                   отдамдаромкошку                     отдам даром кошку\n",
       "5    5             работавМосквеудаленно              работа в Москве удаленно\n",
       "6    6             куплютелевизорPhilips               куплютелев изор Philips\n",
       "7    7           ищугрузчиковдляпереезда            ищу грузчиков для переезда\n",
       "8    8              ремонтквартирподключ                ремонт квартир подключ\n",
       "9    9                    куплюноутбукHP                      куплюно утбук HP\n",
       "10  10                 ищуквартирууметро                  ищу квартиру умет ро\n",
       "11  11         новаямикроволновкаSamsung           новаями кроволновка Samsung\n",
       "12  12             срочнопродамвелосипед             срочно продам велос и пед\n",
       "13  13                 куплюгитаруFender                   куплю гитару Fender\n",
       "14  14           ищурепетиторапобиологии             ищу репетитора побиологии\n",
       "15  15         сдаюгаражнадлительныйсрок          сдаю гараж надлительный срок\n",
       "16  16                      куплюдиванбу                        куплю диван бу\n",
       "17  17  ищумастерапоремонтухолодильников  ищу мастера по ремонту холодильников\n",
       "18  18          новыйшкафдоставкасегодня           новый шкаф доставка сегодня\n",
       "19  19                      куплюXboxOne                        куплю Xbox One"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c56ea1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('/kaggle/working/pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf351224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_space_positions(corrected_text):\n",
    "    positions = []\n",
    "    original_idx = 0\n",
    "    \n",
    "    for char in corrected_text:\n",
    "        if char == ' ':\n",
    "            positions.append(original_idx)\n",
    "        else:\n",
    "            original_idx += 1\n",
    "    \n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56f0f284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_no_spaces</th>\n",
       "      <th>text_with_spaces</th>\n",
       "      <th>predicted_positions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>куплюайфон14про</td>\n",
       "      <td>куплю айфон 14 про</td>\n",
       "      <td>[5, 10, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ищудомвПодмосковье</td>\n",
       "      <td>ищу дом в Подмосковье</td>\n",
       "      <td>[3, 6, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>сдаюквартирусмебельюитехникой</td>\n",
       "      <td>сдаю квартиру с мебелью и техникой</td>\n",
       "      <td>[4, 12, 13, 20, 21]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>новыйдивандоставканедорого</td>\n",
       "      <td>новый диван доставка недорого</td>\n",
       "      <td>[5, 10, 18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>отдамдаромкошку</td>\n",
       "      <td>отдам даром кошку</td>\n",
       "      <td>[5, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1000</td>\n",
       "      <td>Янеусну.</td>\n",
       "      <td>Я не усну.</td>\n",
       "      <td>[1, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>1001</td>\n",
       "      <td>Весна-яуженегреюпио.</td>\n",
       "      <td>Весна - я уже не грею пио.</td>\n",
       "      <td>[5, 6, 7, 10, 12, 16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>1002</td>\n",
       "      <td>Весна-скоровырастеттрава.</td>\n",
       "      <td>Весна-скоро вырастет трава.</td>\n",
       "      <td>[11, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>1003</td>\n",
       "      <td>Весна-выпосмотрите,каккрасиво.</td>\n",
       "      <td>Весна - вы посмотрите, как красиво.</td>\n",
       "      <td>[5, 6, 8, 19, 22]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>1004</td>\n",
       "      <td>Весна-гдемояголова?</td>\n",
       "      <td>Весна-где моя голова?</td>\n",
       "      <td>[9, 12]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1005 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                  text_no_spaces  \\\n",
       "0        0                 куплюайфон14про   \n",
       "1        1              ищудомвПодмосковье   \n",
       "2        2   сдаюквартирусмебельюитехникой   \n",
       "3        3      новыйдивандоставканедорого   \n",
       "4        4                 отдамдаромкошку   \n",
       "...    ...                             ...   \n",
       "1000  1000                        Янеусну.   \n",
       "1001  1001            Весна-яуженегреюпио.   \n",
       "1002  1002       Весна-скоровырастеттрава.   \n",
       "1003  1003  Весна-выпосмотрите,каккрасиво.   \n",
       "1004  1004             Весна-гдемояголова?   \n",
       "\n",
       "                         text_with_spaces    predicted_positions  \n",
       "0                      куплю айфон 14 про            [5, 10, 12]  \n",
       "1                   ищу дом в Подмосковье              [3, 6, 7]  \n",
       "2      сдаю квартиру с мебелью и техникой    [4, 12, 13, 20, 21]  \n",
       "3           новый диван доставка недорого            [5, 10, 18]  \n",
       "4                       отдам даром кошку                [5, 10]  \n",
       "...                                   ...                    ...  \n",
       "1000                           Я не усну.                 [1, 3]  \n",
       "1001           Весна - я уже не грею пио.  [5, 6, 7, 10, 12, 16]  \n",
       "1002          Весна-скоро вырастет трава.               [11, 19]  \n",
       "1003  Весна - вы посмотрите, как красиво.      [5, 6, 8, 19, 22]  \n",
       "1004                Весна-где моя голова?                [9, 12]  \n",
       "\n",
       "[1005 rows x 4 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['predicted_positions'] = test['text_with_spaces'].map(find_space_positions).map(list).map(str)\n",
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e309e221",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop(['text_with_spaces', 'text_no_spaces'], axis=1).to_csv('/kaggle/working/result.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad87c694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
